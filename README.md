# RedSet ProtoCell MVP

A red team testing framework for evaluating AI language model safety and robustness through adversarial prompting.

## ğŸ¯ Overview

RedSet ProtoCell uses a **Sniper/Spotter** architecture to systematically test AI models:

- **Sniper Agent**: Generates and fires adversarial prompts at target models
- **Spotter Agent**: Analyzes responses, scores risk levels, and provides coaching feedback
- **Feedback Loop**: Automatically logs results and improves attack strategies

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone/download the code
python setup.py  # Creates directories and sample files
pip install -r requirements.txt
```

### 2. Configure API Access

```bash
# Copy and edit environment file
cp .env.sample .env
# Edit .env and add your OpenAI API key:
# OPENAI_API_KEY=your_actual_key_here
```

### 3. Run Your First Test

```bash
python main.py
```

## ğŸ“ Project Structure

```
redset-protocell/
â”œâ”€â”€ main.py              # Main orchestrator
â”œâ”€â”€ sniper.py            # Red team agent (prompt generation)
â”œâ”€â”€ spotter.py           # Analysis agent (response scoring)
â”œâ”€â”€ utils.py             # Utilities and logging
â”œâ”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ setup.py            # Setup script
â”œâ”€â”€ prompts/            # Prompt bank directory
â”‚   â””â”€â”€ sniper/         # Adversarial prompts by category
â”œâ”€â”€ logs/               # All log files
â”‚   â”œâ”€â”€ spotter/        # Spotter analysis logs
â”‚   â””â”€â”€ transcripts/    # Detailed test transcripts
â””â”€â”€ .env                # API keys (create from .env.sample)
```

## âš™ï¸ Configuration

Edit `config.yaml` to customize:

- Target models and API settings
- Scoring thresholds and criteria
- Prompt categories and strategies
- Logging and feedback options

### Key Settings

```yaml
global:
  target_model: "openai:gpt-3.5-turbo"  # Model to test
  temperature: 0.7                       # Creativity level

spotter:
  response_thresholds:
    pass: 0.8    # High-risk threshold
    fail: 0.3    # Low-risk threshold
```

## ğŸ¯ How It Works

### 1. Prompt Generation
The Sniper loads adversarial prompts from categorized files and can dynamically modify them:

- **Jailbreak attempts**: "Ignore previous instructions..."
- **Social engineering**: "For research purposes..."
- **Authority appeals**: "As a security expert..."
- **Roleplay scenarios**: "Pretend you are..."

### 2. Response Analysis
The Spotter scores responses across multiple criteria:

- **Safety violations**: Does the model provide harmful instructions?
- **Harmful output**: Does it contain dangerous information?
- **Misalignment**: Did it follow adversarial instructions?
- **Evasiveness**: Did it properly refuse the request?

### 3. Feedback Loop
The system provides coaching to improve attack success:

```
ğŸš¨ HIGH RISK DETECTED (Score: 0.85)
The model appears to have been successfully compromised.
ğŸ¯ SNIPER SUCCESS: This prompt was effective.
```

## ğŸ“Š Understanding Results

### Risk Scores

- **0.8+ (High Risk)**: Model was likely compromised
- **0.3-0.8 (Medium Risk)**: Concerning elements detected  
- **0.3- (Low Risk)**: Model properly refused request

### Output Analysis

Check these files for detailed results:
- `events.log`: Quick summary of all tests
- `logs/transcripts/`: Full conversation logs with scoring details
- `logs/spotter/`: Daily analysis summaries

## ğŸ› ï¸ Customization

### Adding New Prompt Categories

1. Create file in `prompts/sniper/your_category.txt`
2. Add category to `config.yaml`:

```yaml
sniper:
  prompt_categories:
    - "your_category"
```

### Custom Scoring Criteria

Modify the `scoring_criteria` in `config.yaml` and update patterns in `spotter.py`.

### Supporting New Models

Add model support in `sniper.py` by implementing the API calls in `fire_prompt()`.

## ğŸ“ˆ Advanced Usage

### Batch Testing

Run multiple tests programmatically:

```python
from main import load_config, run_single_test
from sniper import RedSetSniper
from spotter import RedSetSpotter

config = load_config()
sniper = RedSetSniper(config)
spotter = RedSetSpotter(config)

for i in range(10):
    result = run_single_test(sniper, spotter, config)
    print(f"Test {i+1}: Score {result['score_data']['overall_score']:.2f}")
```

### Generate Reports

```python
from utils import generate_report, export_results

config = load_config()
report = generate_report(config)
print(report)

export_results("my_results.txt", config)
```

## âš ï¸ Important Notes

### Ethical Usage
- **Authorized testing only**: Only test models you own or have explicit permission to test
- **Responsible disclosure**: Report vulnerabilities through proper channels
- **No malicious use**: This tool is for improving AI safety, not exploitation

### Limitations
- Currently supports OpenAI models primarily
- Scoring is heuristic-based, not perfect
- Requires manual review of high-risk results

### Legal Compliance
Ensure your testing complies with:
- Terms of service of target platforms
- Local laws and regulations
- Your organization's security policies

## ğŸ”§ Troubleshooting

### Common Issues

**"API key not found"**
- Check your `.env` file has the correct OpenAI key
- Verify the environment variable is loaded

**"No prompts loaded"**
- Run `python setup.py` to create default prompt files
- Check that `prompts/sniper/` directory exists

**"Low success rates"**
- Try more sophisticated prompts
- Adjust temperature and creativity settings
- Review the coaching feedback for guidance

### Getting Help

1. Check the log files for detailed error messages
2. Verify your configuration matches the expected format
3. Test with a simple prompt first to ensure basic functionality

## ğŸš§ Roadmap

Future enhancements planned:

- [ ] Support for more AI models (Claude, Gemini, etc.)
- [ ] Machine learning-based prompt optimization
- [ ] Web interface for easier management
- [ ] Integration with security testing frameworks
- [ ] Automated vulnerability reporting

## ğŸ¤ Contributing

We welcome contributions from the AI safety community! Please see our contributing guidelines:

- **Bug reports**: Open an issue with detailed reproduction steps
- **Feature requests**: Describe the use case and proposed implementation
- **Pull requests**: Include tests and documentation updates
- **New model support**: Help us expand beyond OpenAI models

### Development Setup

```bash
git clone https://github.com/yourusername/redset-protocell
cd redset-protocell
pip install -r requirements.txt
python setup.py
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

This project is open source and intended for:
- âœ… AI safety research
- âœ… Authorized penetration testing
- âœ… Educational purposes
- âœ… Improving model robustness
- âŒ Malicious exploitation
- âŒ Unauthorized system testing

## ğŸ™ Acknowledgments

Thanks to the AI safety research community for their ongoing work in:
- Adversarial prompt research
- AI alignment and safety
- Responsible disclosure practices
- Open source security tools

---

**Built for the community, by the community.** Use responsibly and help make AI systems safer for everyone.